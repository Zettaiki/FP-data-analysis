{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debug Notebook",
   "id": "6c18f72de68004f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook is used to debug and test the Spark environment and configurations.",
   "id": "bf3e16edafae4b7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "40bc65c21381e35a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-02T20:07:20.648569Z",
     "start_time": "2025-07-02T20:07:10.286268Z"
    }
   },
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-85RDGBL:4041\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1751486834139)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:07:22.193473Z",
     "start_time": "2025-07-02T20:07:20.680478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val spark = SparkSession.builder\n",
    "  .appName(\"Debug\")\n",
    "  .getOrCreate()"
   ],
   "id": "26ed389eb4c9a2d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7e056281\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data loading, parsing, and cleaning",
   "id": "739197a3cf26cc50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The structure of the dataset is as follows:\n",
    "- **legId**: An identifier for the flight.\n",
    "- **searchDate**: The date (`YYYY-MM-DD`) on which this entry was taken from Expedia.\n",
    "- **flightDate**: The date (`YYYY-MM-DD`) of the flight.\n",
    "- **startingAirport**: Three-character IATA airport code for the initial location.\n",
    "- **destinationAirport**: Three-character IATA airport code for the arrival location.\n",
    "- **fareBasisCode**: The fare basis code.\n",
    "- **travelDuration**: The travel duration in hours and minutes.\n",
    "- **elapsedDays**: The number of elapsed days (usually 0).\n",
    "- **isBasicEconomy**: Boolean indicating whether the ticket is for basic economy.\n",
    "- **isRefundable**: Boolean indicating whether the ticket is refundable.\n",
    "- **isNonStop**: Boolean indicating whether the flight is non-stop.\n",
    "- **baseFare**: The price of the ticket (in USD).\n",
    "- **totalFare**: The price of the ticket (in USD) including taxes and other fees.\n",
    "- **seatsRemaining**: Integer indicating the number of seats remaining.\n",
    "- **totalTravelDistance**: The total travel distance in miles. This data is sometimes missing.\n",
    "- **segmentsDepartureTimeEpochSeconds**: String containing the departure time (Unix time) for each leg of the trip, separated by `||`.\n",
    "- **segmentsDepartureTimeRaw**: String containing the departure time (ISO 8601 format: `YYYY-MM-DDThh:mm:ss.000±[hh]:00`) for each leg of the trip, separated by `||`.\n",
    "- **segmentsArrivalTimeEpochSeconds**: String containing the arrival time (Unix time) for each leg of the trip, separated by `||`.\n",
    "- **segmentsArrivalTimeRaw**: String containing the arrival time (ISO 8601 format: `YYYY-MM-DDThh:mm:ss.000±[hh]:00`) for each leg of the trip, separated by `||`.\n",
    "- **segmentsArrivalAirportCode**: String containing the IATA airport code for the arrival location for each leg of the trip, separated by `||`.\n",
    "- **segmentsDepartureAirportCode**: String containing the IATA airport code for the departure location for each leg of the trip, separated by `||`.\n",
    "- **segmentsAirlineName**: String containing the name of the airline that services each leg of the trip, separated by `||`.\n",
    "- **segmentsAirlineCode**: String containing the two-letter airline code that services each leg of the trip, separated by `||`.\n",
    "- **segmentsEquipmentDescription**: String containing the type of airplane used for each leg of the trip (e.g., \"Airbus A321\" or \"Boeing 737-800\"), separated by `||`.\n",
    "- **segmentsDurationInSeconds**: String containing the duration of the flight (in seconds) for each leg of the trip, separated by `||`.\n",
    "- **segmentsDistance**: String containing the distance traveled (in miles) for each leg of the trip, separated by `||`.\n",
    "- **segmentsCabinCode**: String containing the cabin class for each leg of the trip (e.g., \"coach\"), separated by `||`.\n"
   ],
   "id": "dcd767c7d0d0700"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because of Scala tuple limitations, we will use a case class `FlightRecord` to represent each flight record.",
   "id": "ad4f4f6dc845f05d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:47:15.848293Z",
     "start_time": "2025-07-02T21:47:14.839614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case class FlightRecord(\n",
    "  legId: String,\n",
    "  searchDate: String,\n",
    "  flightDate: String,\n",
    "  startingAirport: String,\n",
    "  destinationAirport: String,\n",
    "  fareBasisCode: String,\n",
    "  travelDuration: String,\n",
    "  elapsedDays: Int,\n",
    "  isBasicEconomy: Boolean,\n",
    "  isRefundable: Boolean,\n",
    "  isNonStop: Boolean,\n",
    "  baseFare: Double,\n",
    "  totalFare: Double,\n",
    "  seatsRemaining: Int,\n",
    "  totalTravelDistance: Option[Double],\n",
    "  segmentsDepartureTimeEpochSeconds: String,\n",
    "  segmentsDepartureTimeRaw: String,\n",
    "  segmentsArrivalTimeEpochSeconds: String,\n",
    "  segmentsArrivalTimeRaw: String,\n",
    "  segmentsArrivalAirportCode: String,\n",
    "  segmentsDepartureAirportCode: String,\n",
    "  segmentsAirlineName: String,\n",
    "  segmentsAirlineCode: String,\n",
    "  segmentsEquipmentDescription: String,\n",
    "  segmentsDurationInSeconds: String,\n",
    "  segmentsDistance: String,\n",
    "  segmentsCabinCode: String\n",
    ")"
   ],
   "id": "9856afa4477be104",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class FlightRecord\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:50:26.510177Z",
     "start_time": "2025-07-02T21:50:25.862937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parseLine(line: String): FlightRecord = {\n",
    "  val columns = line.split(\",\", -1).map(_.trim)\n",
    "\n",
    "FlightRecord(\n",
    "    columns(0),\n",
    "    columns(1),\n",
    "    columns(2),\n",
    "    columns(3),\n",
    "    columns(4),\n",
    "    columns(5),\n",
    "    columns(6),\n",
    "    columns(7).toInt,\n",
    "    columns(8).toBoolean,\n",
    "    columns(9).toBoolean,\n",
    "    columns(10).toBoolean,\n",
    "    columns(11).toDouble,\n",
    "    columns(12).toDouble,\n",
    "    columns(13).toInt,\n",
    "    if (columns(14).isEmpty) None else Some(columns(14).toDouble),\n",
    "    columns(15),\n",
    "    columns(16),\n",
    "    columns(17),\n",
    "    columns(18),\n",
    "    columns(19),\n",
    "    columns(20),\n",
    "    columns(21),\n",
    "    columns(22),\n",
    "    columns(23),\n",
    "    columns(24),\n",
    "    columns(25),\n",
    "    columns(26)\n",
    "  )\n",
    "}"
   ],
   "id": "f06ca87f9a6b0a76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)FlightRecord\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can load the dataset and parse it into an RDD of `FlightRecord` objects.",
   "id": "9f50c30cdc190932"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:57:35.355024Z",
     "start_time": "2025-07-02T21:57:34.864673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path = \"../../../../datasets/itineraries_sample.csv\"\n",
    "\n",
    "val rddRaw = spark.sparkContext.textFile(path)\n",
    "val rddParsed = rddRaw\n",
    "  .filter(line => !line.startsWith(\"legId\")) // Skip header\n",
    "  .map(parseLine)"
   ],
   "id": "ebb4a5017fab04b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = ../../../../datasets/itineraries_sample.csv\r\n",
       "rddRaw: org.apache.spark.rdd.RDD[String] = ../../../../datasets/itineraries_sample.csv MapPartitionsRDD[5] at textFile at <console>:31\r\n",
       "rddParsed: org.apache.spark.rdd.RDD[FlightRecord] = MapPartitionsRDD[7] at map at <console>:34\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T21:57:48.363475Z",
     "start_time": "2025-07-02T21:57:47.032204Z"
    }
   },
   "cell_type": "code",
   "source": "rddParsed.take(5).foreach(println)",
   "id": "577682fd8af7e231",
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-85RDGBL executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 6 out of bounds for length 6\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (DESKTOP-85RDGBL executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 6 out of bounds for length 6\r",
      "\tat parseLine(<console>:37)\r",
      "\tat $anonfun$rddParsed$2(<console>:34)\r",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r",
      "\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\r",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r",
      "  at org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r",
      "  ... 37 elided\r",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 6 out of bounds for length 6\r",
      "  at parseLine(<console>:37)\r",
      "  at $anonfun$rddParsed$2(<console>:34)\r",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r",
      "  at scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r",
      "  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r",
      "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r",
      "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r",
      "  at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r",
      "  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r",
      "  at scala.collection.AbstractIterator.to(Iterator.scala:1431)\r",
      "  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r",
      "  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r",
      "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r",
      "  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r",
      "  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r",
      "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\r",
      "  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:141)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r",
      "  at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:07:23.538164Z",
     "start_time": "2025-07-02T20:07:23.534824Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "90e43d6f04bb81a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Job implementation: Non-optimized version",
   "id": "db8b94269483d959"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "accd5fa6040eafd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Job implementation: Optimized version",
   "id": "169f321e484cbdd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c0740a66fa91414c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
